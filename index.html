<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>index</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}
</style>


<script src="index_files/libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="index_files/libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="index_files/libs/datatables-binding-0.31/datatables.js"></script>
<script src="index_files/libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="index_files/libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="index_files/libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="index_files/libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="index_files/libs/dt-ext-responsive-1.13.6/css/responsive.dataTables.min.css" rel="stylesheet">
<script src="index_files/libs/dt-ext-responsive-1.13.6/js/dataTables.responsive.min.js"></script>
<link href="index_files/libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet">
<script src="index_files/libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="index_files/libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet">
<script src="index_files/libs/selectize-0.12.0/selectize.min.js"></script>
<link href="index_files/libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="index_files/libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>


</head>

<body>





<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-f7a48c6faab9d54ddecc" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-f7a48c6faab9d54ddecc">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"1\" data-max=\"47\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","extensions":["Responsive"],"data":[[33,4,23,9,16,35,30,15,26,46,34,8,14,24,11,36,6,22,28,10,17,45,40,25,27,7,29,32,38,3,42,41,43,12,2,5,19,21,44,1,18,31,37,39,13,20,47],["Thurs, April 18","Wed, April 17","Thurs, April 18","Wed, April 17","Wed, April 17","Thurs, April 18","Thurs, April 18","Wed, April 17","Thurs, April 18","Fri, April 19","Thurs, April 18","Wed, April 17","Wed, April 17","Thurs, April 18","Wed, April 17","Thurs, April 18","Wed, April 17","Thurs, April 18","Thurs, April 18","Wed, April 17","Wed, April 17","Fri, April 19","Fri, April 19","Thurs, April 18","Thurs, April 18","Wed, April 17","Thurs, April 18","Thurs, April 18","Thurs, April 18","Wed, April 17","Fri, April 19","Fri, April 19","Fri, April 19","Wed, April 17","Wed, April 17","Wed, April 17","Wed, April 17","Thurs, April 18","Fri, April 19","Wed, April 17","Wed, April 17","Thurs, April 18","Thurs, April 18","Fri, April 19","Wed, April 17","Thurs, April 18","Fri, April 19"],["14:20 – 14:40","10:35 – 10:55","10:05 – 10:25","12:10 – 12:30","15:15 – 15:35","14:20 – 14:40","12:05 – 12:25","15:40 – 16:00","11:40 – 12:00","11:10 – 11:30","14:45 – 15:05","11:45 – 12:05","15:15 – 15:35","10:30 – 10:50","11:45 – 12:05","14:45 – 15:05","10:35 – 10:55","10:30 – 10:50","11:15 – 11:35","11:20 – 11:40","15:40 – 16:00","10:45 – 11:05","9:35 – 9:55","11:15 – 11:35","12:05 – 12:25","11:20 – 11:40","11:40 – 12:00","13:50 – 14:10","16:05 – 16:25","10:10 – 10:30","10:45 – 11:05","10:20 – 10:40","11:10 – 11:30","12:10 – 12:30","9:40 – 10:00","10:10 – 10:30","17:00 – 17:20","10:05 – 10:25","10:20 – 10:40","9:05 – 9:35","16:25 – 16:55","13:15 – 13:45","15:30 – 16:00","9:00 – 9:30","14:05 – 15:05","9:00 – 10:00","11:40 – 12:40"],["A","A","B","A","B","B","B","A","A","B","A","A","A","B","B","B","B","A","B","B","B","B","A","A","A","A","B","A","A","A","A","A","A","B","A","B","A","A","B","A","A","A","A","A","A","A","A"],["Covariate Selection in Instrumental Variable Models","Causal Misspecification in Bayesian Inference","Sick of Caring? The impact of hospital employment on health and long-term sick leave","Causal Inference Under Local Differential Privacy","Interval identification of natural effects in the presence of outcome-related unmeasured confounding","The choice of utility functions for optimal individualized regimes","Synthetic Difference-in-Differences for Multiplicative Models","Efficient estimation of modified treatment policy effects based on the generalized propensity scor","Comparing theory-driven and data-driven approaches to constructing causal models for life course epidemiology","Flexible Machine Learning Estimation of Conditional Average Treatment Effects: A Blessing and a Curse","Estimation of causal effects using covariate adjustment in vector autoregressive processes","Simulating data from marginal structural models for a survival time outcome","Bridging the gap between marginal and conditional estimands: advancing debiased machine learning methods for improved interpretation and decision-making in RCTs","Semi-competing risks approach for studying causal effects of antibiotic treatment on future collateral resistance","Navigating treatment discontinuation in emulated trials: A closer look at the intention-to-treat approach","Improving efficiency of inference in randomised controlled trials by leveraging historical control data","Irregular measurement times in Marginal Structural Models: Categorizing biases and comparing adjustment methods","Uniformly valid causal inference and unobserved confounding in high-dimensional settings","(Marginal) Structural Nested Mean Models Under Parallel Trends","Combining information from trial participants and non-participants in registry-based trials","Long Story Short: Omitted Variable Bias in Causal Machine Learning","Causal Learning with Deep Latent Variable Methods: Leveraging Noisy Confounder Proxies to Estimate the Effects of Antibiotics on Asthma Exacerbations in Children","Negative Control tests for Instrumental Variable Designs","Post-selection inference for causal effects after causal discovery","Confidence in Causal Inference under Structure Uncertainty","Imperfect Data and Optimal Allocation of Data-collection Resources","Correcting invalid regression discontinuity designs with multiple time period data","Quality of Life while alive - Challenges and opportunities in evaluating treatment effects from Single Arm Trial and External Control data","Efficient adjustment for complex covariates: Gaining efficiency with DOPE","Bayesian Nonparametrics for Principal Stratification: an Application on Environmental Policies Effects on Health","A General Design-Based Framework and Estimator for Randomized Experiments","Transporting Policy Effects under Interference and Effect Heterogeneity","Random network in time series studies with bipartite interference","Construction of a causal inference framework for a semi-Markovian multi-state model for drug-repurposing","Bayesian Causal Forests Combining Randomised And Observational Data For Heterogeneous Treatment Effects Estimation","The MedWeight Study: Comparative effects of time-varying treatment adherence when outcomes are informatively measured, continuous health markers in EHR data","Recovering target causal effects from post-exposure selection induced by missing outcome data","Improved bounds and inference on optimal regimes","Change of treatment schedules and Neyman orthogonalization in counting process models.","Bayesian reasoning about causal estimands","What is a good imputation under MAR missingness?","Optimizing Randomized Trial Efficiency: Innovative Approaches to Covariate Adjustment","Causal effects in a Markov equivalence class: Identification and efficient estimation","Testing identification in mediation and dynamic treatment models","The Causal Roadmap in the Age of AI: From All Wheel Drive to Formula 1","Ding and VanderWeele’s sensitivity analysis: review, improvements and limitations","TBD"],["Linbo Wang","Lisa Wimmer","Louise Paaske","Máté Kormos","Marco Doretti","Mats Stensrud","Nicolas Apfel","Nima Hejazi","Anne Helby Petersen","Richard Post","Søren Wengel Mogensen","Shaun Seaman","Stijn Vansteelandt","Tamir Zehavi","Thomas Gerds","Wout Waterschoot","Wouter Kant","Xavier de Luna","Zach Shahn","Camila Olarte Parra","Carlos Cinelli","Albina Tskhay","Daniel Nevo","Daniel Malinsky","David Strieder","Dean Knox","Dor Leventer","Dries Reynders","Alexander Mangulad Christgau","Falco J. Bargagli Stoffi","Fredrik Sävje","Gary Hettinger","Georgia Papadogeorgou","Alexandra Kortchemski","Ilina Yozova","Jessica Young","Johan de Aguas","Julien Laurendeau","Kjetil Røysland","Chris Holmes","Julie Josse","Kelly Van Lancker","Emilija Perkovic","Martin Huber","Maya Petersen","Arvid Sjölander","Elizabeth Ogburn"],["University of Toronto","LMU Munich","Danish Centre for Health Economics, University of Southern Denmark","Delft Institute of Technology","University of Florence","Ecole Polytechnique Federale de Lausanne (EPFL)","University of Southampton, UK","Harvard T.H. Chan School of Public Health","University of Copenhagen","Eindhoven University of Technology","Lund University","University of Cambridge","Ghent University","Tel Aviv University","University of Copenhagen","Ghent University","Radboud University","Umeå University","CUNY School of Public Health","CAUSALab, Unit of Epidemiology, Institute of Environmental Medicine, Karolinska Universitet","University of Washington","Department of Family Medicine, McGill University; CHU Saint-Justine Research Center.","Tel Aviv University","Columbia University","Technical University of Munich/MCML","Wharton","Tel Aviv University","Ghent University","University of Copenhagen","Harvard University","Yale University &amp; Uppsala University","University of Pennsylvania","University of Floriday","Massachusetts General Hospital/Harvard Medical School, CentraleSupélec (Paris-Saclay University)","University College London","Harvard Medical School &amp; Harvard Pilgrim Health Care Institute","University of Oslo / Norwegian Institute of Public Health","EPFL","University of Oslo","University of Oxford","Inria (National research center in digital science)","Ghent University","University of Washington"," University of Fribourg, Switzerland","University of California, Berkeley","Karolinska Institutet","Johns Hopkins University"],["Chuyun Ye, Dingke Tang","Ludwig Bothmann","Nicolai Damslund, Anne Helene Garde, Ann Dyreborg Larsen, Kim Rose Olsen","Stéphanie van der Pas, Aad van der Vaart","Elena Stanghellini (University of Perugia)","Aaron Sarvet","Joao Santos Silva, Holger Breinlich, Valentina Corradi (Uni Surrey, UK), Tom Zylkin (Uni Richmond, US)","Mark van der Laan, David Benkeser, Ivan Diaz","Claus Thorn Ekstrøm, Peter Spirtes, Merete Osler","Marko Petkovic",null,"Ruth Keogh","Kelly Van Lancker, Henrik Ravn and Jesper Madsen","Daniel Nevo","Anders Munch","Andrea Callegaro, Stijn Vansteelandt","Jesse H. Krijthe","Niloofar Moosavi, Tetiana Gorbach","Oliver Dukes, James Robins, Andrea Rotnitzky","Anthony Matthews, Conor Macdonald, Anita Berglund, Issa Dahabreh","Victor Chernozhukov, Whitney Newey, Amit Sharma, Vasilis Syrgkanis","Tibor Schuster (Department of Family Medicine McGill University); Crisitna Longo (CHU Saint-Justine Research Center; Faculty of Pharmacy, University of Montreal)","Oren Danieli, Itai Walk, Bar Weinstein, Dan Zeltzer","Ting-Hsuan Chang, Zijian Guo","Mathias Drton","Guilherme Duarte, Kai Cooper","Daniel Nevo","Doranne Thomassen (LUMC), Satrajit Roychoudhury (Pfizer Inc.), Cecilie Delphin Amdal (UHOslo), Jammbe Z. Musoro (EORTC), Willi Sauerbrei (UFreiburg), Saskia le Cessie (LUMC), Els Goetghebeur (UGent); on behalf of SISAQOL-IMI Work Package 3","Niels Richard Hansen","Fabrizia Mealli, Francesca Dominici, Dafne Zorzetto, Antonio Canale","Christopher Harshaw, Yitan Wang","Youjin Lee, Nandita Mitra","Zhaoyan Song","Marie-Laure Charpignon (MIT), Thibaut Horel (MIT), Colin Magdamo (Massachusetts General Hospital/Harvard Medical School), Bella Vakulenko-Lagun (Haifa University), Mark W. Albers(Massachusetts General Hospital/Harvard Medical School)","Ioanna Manolopoulou","Sheryl Rifas-Shiman, Han Yu, Joshua Petimar, Jason Block","Johan Pensar, Tomás Varnet, and Guido Biele","Aaron L. Sarvet and Mats J. Stensrud",null,null,null,null,null,null,null,null,null],["The instrumental variable (IV) framework is widely used in practice to address unmeasured confounding. In contemporary datasets, rich covariate information is often available in addition to the instrument. To build tractable IV models, it is essential to first extract important features from the high-dimensional covariates. While much of the literature has focused on variable selection for predictive modeling and confounder selection in the absence of unmeasured confounding, in this talk, we introduce the covariate selection problem within the IV framework. Like the confounder selection process, the goal here is to minimize residual confounding bias while improving efficiency in the resulting causal effect estimator. Through extensive simulations and theoretical analyses, we establish parallels between the roles of covariates in IV covariate selection and in confounder selection, paving the way for future endeavors in this area.","This work explores a novel perspective on causality and Bayesian inference. We study not the role of uncertainty in causal inference but the implications of causal relations for the estimation of (predictive) uncertainty in machine learning (ML) tasks. Our approach challenges the ML paradigm of refraining from making assumptions about the data-generating process and betting everything on the observed data instead. While this tends to work well in terms of predictive performance, several scholars have noted undesirable phenomena, with models fooled into detecting spurious relations, that can be attributed to the total disregard of causal relationships. We conjecture that the consequences of anti-causal modeling on uncertainty quantification (UQ) might be similarly detrimental. UQ has recently become an active field of research, most notably in the deep learning community rife with powerful models that are notoriously ill-suited to reporting their own predictive uncertainty. The UQ literature tends to operate under the assumption that the true model can be uncovered with enough data. It is, however, not at all clear how uncertainty estimates are to be interpreted under what we call causal misspecification, i.e., when the learned function fundamentally differs from the underlying causal model (through, say, unobserved confounders). We analyze how data situations violating the causal truth affect the faithfulness of predictive UQ, an endeavor severely complicated by the absence of ground-truth uncertainty. We address this point with simulation-based studies in which we characterize scenarios of causal misspecification and assess the resulting behavior of uncertainty estimates therein.","Hospitals around Europe face increasing problems with recruitment and retention. This indicates that working in hospitals is less attractive compared to other industries. It is well known that occupation is highly associated with health, but the causal effect of specific job exposures is difficult to study because of selection into occupation. In this paper we use a novel approach, emulating a hypothetical target trial by leveraging rich register data to examine the causal impact of hospital employment on mental health, back disease, and long-term sick leave. The first step of our empirical strategy is to identify an inception cohort of newly educated individuals with equal educational attainment entering the job market before age 30 and without prior contamination of exposure or health outcomes. The population is nurses and midwifes employed at a Danish hospital derived from national hospital payroll data and the control population consists of individuals with other occupations employed in different industries. Second, we match exact on age, sex, marital status, non-Danish ethnicity, number of children, municipality of residence and employment year. Third, we apply a fixed effect event study model to control for time invariant unobserved confounding and bias related to selection into occupation. We find that hospital employed nurses and midwifes face significantly fewer mental health conditions, more back diseases and higher occurrences of long-term sick leave compared to the matched control group.","We consider estimation of average treatment effects of a binary treatment from observational data in the presence of a categorical covariate that ensures unconfoundedness and is protected by local differential privacy. \n\n\nFirst, we study estimators rooted in regression-adjustment. We show that ignoring the privatisation leads to biased estimates, and construct bias-corrected estimators that are asymptotically unbiased and normal drawing on the knowledge of the privacy mechanism. \n\nSecond, we turn towards weighting estimators of average treatment effects, which, in a nonprivate setting, would use the propensity score. We show that simply plugging in the privatised covariate in the propensity score weighting estimator leads to a bias. Instead, we propose a weighting estimator that corrects this bias. This estimator is also proved to be asymptotically unbiased and normal.\n\n\nRegrettably, the analytical comparison of the asymptotic variances of the two types of estimators remains difficult. In order to construct asymptotically efficient estimators based on the efficient influence function, we analyse the semiparametric efficiency properties of our private causal model. This is ongoing research.","With reference to a binary outcome and a binary mediator, we derive identification bounds for natural effects under a reduced set of assumptions. Specifically, no assumptions about confounding are made that involve the outcome; we only assume no unobserved exposure-mediator confounding as well as a condition termed partially constant cross-world dependence (PC-CWD). We show how such a condition poses fewer constraints on the counterfactual probabilities than the usual cross-world independence assumption. The proposed strategy can be used also to achieve interval identification of the total effect, which is no longer point identified under the considered set of assumptions. Our derivations are based on postulating a logistic regression model for the mediator as well as for the outcome. However, in both cases the functional form governing the dependence on the explanatory variables is allowed to be arbitrary, thereby resulting in a semi-parametric approach. We provide delta-method approximations of standard errors in order to build uncertainty intervals from identification bounds to account for sampling variability. The proposed method is applied to a dataset gathered from a Spanish prospective cohort study. The aim is to evaluate whether the effect of smoking on lung cancer risk is mediated by the onset of pulmonary emphysema.","Individualizing treatments is justifiable if it results in better (expected) utility for the population. Thus, specifying utility functions—the mapping of an individual's outcomes and other features to some number—forms the basis for determining preferences among treatment options. However, explicit consideration of utilities is often avoided in the statistics and causal inference literature, perhaps because of its philosophical and subjective reputation. Yet, decision-makers and methodologists will usually agree, at least superficially, with the classical Hippocratic maxim: \"First, do no harm.\" An emerging literature offers an operationalization of \"harm\" that promises to possibly 'provide the key for next-generation personalized decision-making' (Mueller and Pearl, 2023), see also Ben Michael et al. (2022). Their methods propose strategies for supplementing experimental data with confounded non-experimental data that are often disregarded in classical approaches. We demonstrate that their proposals take for granted a particular operationalization of \"harm,\" and we contrast it with notions of harm defined by alternative parameters, which can be identified with assumptions that are empirically testable. To bring clarity, we formally articulate two approaches, which we refer to as counterfactual and interventionist, respectively. These two approaches are distinguished by utility functions of fundamentally different natures, with concrete implications for practical decision-making. In particular, they each correspond to different notions of \"harm\" and to different interpretations of the Hippocratic principle. We apply our results to study two previous examples from the optimal regimes literature, illustrating that the different approaches lead to different decisions.","The Synthetic Control Method (SCM) is used to weigh the outcomes of control units to fit the pre-treatment trajectory of the outcome variable of a treated unit as closely as possible. The underlying models that are estimated with the help of the SCM are linear. The numerous log-log or log-level specifications used in economic studies stand testimony that multiplicative models dominate many fields of research. Log-linearizing these models introduces bias in the coefficients of interest. This paper combines state-of-the art SCM, the Synthetic Difference-in-Difference method (SDID) with Poisson Pseudo-Maximum Likelihood (PPML) yielding the SDPP estimator. We show in simulations that it improves performance as compared to OLS, SDID and PPML. We then illustrate its usefulness by applying it to the estimation of the wage effect of the Mariel Boatlift in Miami in the 1980s.","Continuous treatment variables have posed a significant challenge in causal inference, especially for the identification and robust estimation of candidate causal effect definitions. Focus has traditionally been placed on causal effect parameters defined for binary or categorical treatment variables with relatively few levels, allowing for the straightforward application of propensity score-based methodology. Efforts to accommodate continuous-valued treatment variables led to the formulation of causal effects based on modified treatment policies and introduced the generalized propensity score. Yet, most common estimators of this key nuisance parameter rely upon restrictive modeling strategies that further exacerbate the challenging tradeoff between robustness (against model misspecification) and asymptotic efficiency of inverse probability weighted or doubly robust estimation strategies. We reformulate a nonparametric estimator of the generalized propensity score based on the highly adaptive lasso, a recently proposed nonparametric regression procedure, and study the suitability of its rate-convergence properties for asymptotically efficient estimation of modified treatment policy effects. Using this novel estimator, we propose a class of non-restrictive inverse probability weighted estimators capable of achieving asymptotic efficiency in a large, nonparametric statistical model when a suitable sieve-based selection approach is applied for undersmoothing of the generalized propensity score estimator. We further extend this approach to study the performance of doubly robust estimators when the outcome regression is challenging to estimate well. In numerical experiments, we show that these inverse probability weighted estimators are capable of achieving the semiparametric efficiency bound in a setting with continuous treatments and that they compare favorably against their doubly robust counterparts.","Epidemiologists often need to specify complex causal models, e.g. DAGs, before they can estimate causal effects of interest. Especially in life course epidemiology, where the same individuals are followed for long periods of time, constructing such models can be a cumbersome and difficult activity. Moreover, one must refer to established empirical or theoretical results to construct the DAGs, which gives the procedure little room for uncovering new causal pathways: It is fundamentally confirmatory. \n\nCausal discovery is a data-driven alternative to this traditional approach. Here, statistical algorithms are applied to empirical data to deduce what information about the causal data generating mechanism can be recovered. Depending on what assumptions we are willing to make about this mechanism – e.g., no unobserved confounding – the resulting output can be more or less informative.\n\nWe compare data-driven and theory-driven approaches to constructing causal models. We focus on a study concerning etiology of depression and heart disease in early old age for a cohort of Danish men followed from birth (1953) until 2018. We construct models both with and without assuming no unobserved confounding. For the theory-driven approach, epidemiological experts construct the causal models (represented as DAGs/ADMGs). For the data-driven approach, we apply temporal versions of the PC algorithm (assuming no unobserved confounding) and the FCI algorithm (allowing for arbitrary unobserved confounding) to the cohort data. \n\nWe compare the resulting suggestion for causal models and use this application example to discuss when and how causal discovery algorithms may be useful in life course epidemiology and beyond.","Causal inference from observational data requires untestable identification assumptions. If these assumptions apply, machine learning (ML) methods can be used to study complex forms of causal effect heterogeneity. Several ML methods were recently developed to estimate the conditional average treatment effect (CATE). If observed features cannot explain all heterogeneity, the individual treatment effects (ITEs) can still seriously deviate from the CATE. In this talk, we will illustrate the possible difference between the ITE distribution and the individualized CATE distribution by presenting scenarios with varying conditional ITE variance. If the distribution of the ITE equals that of the CATE, the observed difference in conditional variance between treated and controls should be small. If they differ, an additional causal identifiability assumption is necessary to quantify the heterogeneity not captured by the distribution of the CATE. The conditional variance of the ITE can be identified when the ITE is independent of the outcome under no treatment given the measured features. Under this assumption, we extend the causal random forest algorithm to illustrate how ML methods may be used to estimate the conditional variance of the ITE from observational data. For the scenarios where the ITE and CATE distributions differ, we show that the extended causal random forest can appropriately estimate the variance of the ITE distribution, while the traditional causal random forest fails to do so. Finally, we will discuss the impact of the violation of the untestable identifiability assumption on the performance of the extended random forest.","Abstract: In a causal linear model with a finite number of variables, linear regression can be used to estimate total causal effects when the variables included in the linear regression, the adjustment set, satisfy a set of assumptions concerning the causal graph. We extend adjustment theory to vector autoregressive processes (VAR-processes) to obtain analogous results. In our VAR-process causal graphs, each node represents a coordinate process, not a single random variable. This means that we do not need to know the order of the VAR-process, and it could even be of infinite order. \n\nWe define the total causal effect of process A on process B as a sequence of lag-specific causal effects, and we give sufficient graphical conditions for a set of coordinate processes to be an adjustment set for a total causal effect. When these conditions are met, fitting a VAR-model to the observed data will be consistent for the total causal effect using the adjustment set as covariate processes in the VAR-model. When the true order is unknown, a consistent estimator can be formulated as a limit of finite-order estimators. Several adjustment sets may exist, and we discuss efficiency in terms of asymptotic variance of the associated estimators. \n\nWith knowledge of the causal graph, one may use the theory described above to corroborate or invalidate a causal interpretation of parameter estimates obtained using standard VAR-modeling techniques. This is useful in fields where VAR-processes are common in applications, e.g., economics, and we give an example of an application.","Marginal structural models (MSMs) are often used to estimate causal effects of treatments on survival time outcomes from observational data when time-dependent confounding may be present. They can be fitted using, for example, inverse probability of treatment weighting (IPTW).\n\nIt is important to evaluate the performance of statistical methods in different scenarios, and simulation studies are a key tool for such evaluations. In such simulation studies, it is common to generate data in such a way that the model of interest is correctly specified, but this is not always straightforward when the model of interest is for potential outcomes, as is an MSM. Methods have been proposed for simulating from MSMs for a survival time outcome, but these methods impose restrictions on the data-generating mechanism.\n\nWe shall propose a method that overcomes these restrictions. The MSM can be, for example, a marginal structural logistic model for a discrete survival time or a Cox or additive hazards MSM for a continuous survival time. It can also be a continuous-time MSM, i.e. a model for data where an individual's confounders and treatment can change at individual-specific random times.\n\nWe shall illustrate the use of the proposed simulation algorithm by carrying out a brief simulation study. This study compares the coverage of confidence intervals calculated in two different ways for causal effect estimates obtained by fitting an MSM via IPTW.","The conventional reliance on odds ratios and hazard ratios as effect measures in randomized trial analyses that adjust for baseline covariates has faced substantial criticism due to challenges in interpretation, communication, and the potential for model misspecification bias. In response, there has been a surge in the development of model-free estimands for marginal treatment effects, employing data-adaptive statistical modeling and machine learning to correct for random covariate imbalances between treatment arms. However, these developments have themselves been criticized for neglecting the lack of representativeness of many trial participants, and for lacking immediate applicability to patient-centered decision-making.\n\nIn this talk, we will address these concerns by introducing a new class of semi-parametric models for the conditional (counterfactual) outcome means at each treatment level, given covariates, which make minimal parametric assumptions on the distribution of these means. Model parameters encode the population mean treatment effect and its heterogeneity, which may in turn be used for population decision-making, e.g., by regulators. Debiased machine learning is used for estimation, such that inference for those parameters is robust against model misspecification. A further strength of the chosen model class is that it enables easy-to-communicate visualizations of quantiles, such as the median, depicting patient-specific treatment effects in function of the treatment-free risk or mean. The proposed framework thereby supports both population-level decisions as well as more refined individual treatment decisions, while addressing limitations associated with communication to non-statisticians and bias associated with model misspecification. It will be illustrated using randomized clinical trial data.","Comparing future antibiotic resistance resulting from different antibiotic treatments is complicated by the fact that some patients may only survive under the stronger antibiotic. We embed this problem within a semi-competing risks approach, where a resistant bacterial infection is a non-terminal event that might be precluded by the terminal event, i.e. by death. We argue that existing principal stratification estimands for such problems exclude patients for which a causal effect is well-defined and of clinical interest. Therefore, we propose a new principal stratification estimand, the feasible-infection causal effect (FICE), which is the effect among patients that would have been infected or would have survived within one year under both antibiotic treatments. This subpopulation is more inclusive than previously-proposed subpopulations. We develop a partial identification approach in the form of large-sample bounds under novel assumptions, and discuss the plausibility of these assumptions in our application. As an alternative, we derive FICE identification using two illness-death models with a bivariate frailty random variable. The two antibiotic treatment-specific models are then connected by\na cross-world correlation parameter. Because our data is observational, we extend both approaches to account for measured confounding. Estimation is performed by an expectation-maximization algorithm, followed by Monte Carlo simulations. We present simulation results, demonstrating the importance of considering the suggested subpopulation and the advantages of our approach. We apply our method to data collected from a medium-sized hospital, including patients with a bacterial culture during the years 2015 and 2022.","The term \"intention-to-treat\" is commonly employed to describe real world data analyses that contrast initial treatments when emulating a target trial. The estimands of these analyses are often lacking detailed definitions. Notably, in observational studies based on medical and pharmacy claims data, treatment discontinuation occurs more frequently and for different reasons compared to non-compliance in randomized trials. In this presentation, I will review the motivation and significance of intention-to-treat analyses for emulated trials. I will assert that, once fully specified, these analyses target estimands that are high hanging fruits. This is essentially because they require careful understanding of the reasons for treatment discontinuation in the specific target trial, and, contrary common conception, modeling of the treatment discontinuation pattern in the observed data. To illustrate this discussion, I will draw upon our ongoing research on glucose-lowering drugs utilizing nationwide Danish register data.","Randomised controlled trials (RCTs) are regarded as the gold standard to infer treatment effects. In trials of small sample sizes, efficiency of inference is however often limited. With the large availability of historical trials, there has been a growing interest to augment the control arm of the current trial with historical controls. Whilst this may be advantageous in view of efficiency, one then also risks to introduce bias in the treatment effect estimator. Many methods in various strands of the literature have been proposed to leverage external data to a degree that obeys a certain optimality criterion. In this talk, I will focus on two well-known methods. The robust mixture prior approach, a Bayesian dynamic borrowing strategy approved by the FDA, minimises the Bayes risk of the corresponding estimator w.r.t. a specific prior. Another appealing approach estimates treatment effect as a contrast of the outcome mean in treated and controls where the latter is estimated by minimising the empirical MSE of a convex combination of an estimator for the current control outcome mean and an estimator for the historical control outcome mean, with convex combination weight λ, λ ∈ [0,1]. We point out that the well-known literature-based expression for λ is in fact not optimal. In view of this, we study Bayes optimal estimators. We assess the performance of these two methods w.r.t. both measures via extensive simulations and through re-analysis of a prophylactic human papillomavirus vaccine (HPV) efficacy trial (NCT00128661) by leveraging historical control data from NCT00122681.","When using Inverse Probability of Treatment Weighting (IPTW) and Marginal Structural Models to estimate the causal effects of time-varying treatments, it is known that irregularly timed measurements can induce both confounding and selection bias. Two reweighting methods have been proposed to adjust for these biases: Time-as-confounder and reweighting by measurement time. However, it is thus far not well understood in which situations these irregularly timed measurements induce bias, and how the available reweighting methods compare to each other in different situations. In this work, we provide a complete inventorization of all possible backdoor paths through which bias is induced. Based on these paths, we can distinguish three categories of confounding bias by measurement time: Direct confounding, indirect confounding through measured variables, and indirect confounding through unmeasured variables. These categories differ in the assumptions and reweighting methods that can be used to adjust for bias, and can occur simultaneously with selection bias. Through simulation studies, we illustrate: 1. Reweighting by measurement time may be used to adjust for selection bias and confounding through measured variables; 2. Time-as-confounder may be used to adjust for all categories of confounding bias, but not selection bias; and 3. In some cases, a combination of both techniques may be used to adjust for both confounding and selection bias. Properly adjusting for measurement times is crucial in order to avoid unnecessary bias in causal effect estimation, and the categorization of biases and techniques that we introduce can help researchers to make the appropriate adjustments in their analyses.","Various methods have been proposed to estimate causal effects with confidence intervals that are uniformly valid over a set of data generating processes. This is in particular important when nuisance models are estimated by post-model-selection or machine learning estimators. These methods typically require that all the confounders are observed to ensure identification of the effects. We contribute by showing how valid semiparametric inference can be obtained in the presence of unobserved confounders and high-dimensional nuisance models. We propose uncertainty intervals which allow for unobserved confounding, and show that the resulting inference is valid when the amount of unobserved confounding is small relative to the sample size; the latter is formalized in terms of convergence rates. Simulation experiments illustrate the finite sample properties of the proposed intervals and investigate an alternative procedure that improves the empirical coverage of the intervals when the amount of unobserved confounding is larger. Finally, a case study on the effect of smoking during pregnancy on birth weight is used to illustrate the use of the method to perform a sensitivity analysis to unobserved confounding.","We link and extend two approaches to estimating time-varying treatment effects on repeated continuous outcomes—time-varying Difference in Differences (DiD) and Structural Nested Mean Models (SNMMs). In particular, we show that SNMMs are identified under a generalized version of the parallel trends assumption typically used to justify time-varying DiD methods. Because SNMMs model a broader set of causal estimands, our results allow practitioners of existing time-varying DiD approaches to address additional types of substantive questions (such as characterization of time-varying effect heterogeneity, estimation of the lasting effects of a “blip” of treatment at a single time point, and others) under similar assumptions. SNMMs also naturally accommodate continuous and multidimensional treatments, and can be used to identify effects of sustained treatment regimes even when treatment switches on and off in the data, a source of consternation in the DiD literature. We further introduce marginal SNMMs (mSNMMs) that enable modeling effect modification by a subset of covariates required as adjustment variables. Modeling effect modification by lower dimensional covariates reduces the risk of blip model misspecification and can improve interpretability. We illustrate some advantages of the methodology by applying mSNMMs to estimate how the effects of Medicaid expansion (a time-varying treatment) on local uninsurance rates vary with eligibility thresholds at time of expansion (a time-varying covariate) adjusting for additional time-varying demographic and economic variables, a question that could not be addressed using alternative time-varying DiD methods.","The strengths of randomized trials for estimating treatment effects are well understood, but trials often enrol selected populations and have high cost. These drawbacks can be overcome by designing trials nested in registries and using the registry infrastructure to collect information on both trial participants and non-participants. Here, we discuss the identifiability conditions that allow combining information from trial participants and non-participants in registry-based trials to estimate the effects of the randomized treatments, taking into account selection into the trial, assignment into treatment groups, adherence to the assigned treatment, and loss to follow up. We propose estimators that jointly use the randomized and observational data in registry-based trials under different identifiability conditions and we illustrate the use of the estimators using data from a major cardiovascular trial nested in a large national registry.","We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example,(weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated),(weighted) average derivatives, and policy effects from shifts in covariate distribution--all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (eg, average treatment effects and avearage derivatives) the bound is shown to depend on easily interpretable quantities that measure the explanatory power of the omitted variables. Therefore, simple plausibility judgments on the maximum explanatory power of omitted variables (in explaining treatment and outcome variation) are sufficient to place overall bounds on the size of the bias. Furthermore, we use debiased machine learning to provide flexible and efficient statistical inference on learnable components of the bounds. Finally, empirical examples demonstrate the usefulness of the approach.","Causal inference in observational studies requires that all potential confounders are measured and appropriately accounted for either by the research design and/or when analyzing the data. In rich data settings such as observational studies employing medical or health records, measurable features that are causal descendants of unmeasured confounders, but are not causal actors in the exposure-outcome conduit. Recent developments in Deep Latent Variable Modeling suggest that measurement of such (noisy) proxy variables can effectively minimize confounding bias, provided that the joint distribution of exposure, confounders, proxy variables and the outcome can be consistently estimated from the data.\nIn our research, we expand on recently reported studies pertaining the performance of Causal Effect Variational Autoencoder (CEVAE) in estimating individual level causal effects.\nWe are conducting a comprehensive plasmode simulation study assessing the individual causal effect of early life antibiotic exposure on the risk of exacerbation in children diagnosed with asthma. We assess bias and estimate precision as well as computational aspects of various proposed estimators from the literature, including neural networks and state-of-the art ensemble learning algorithms.\nThe anticipated results promise insight on the practical data requirements, neural network/learner architectures and computational resources needed to accurately estimate individual level and average causal effects in a real-world data context.] This project serves as a practical demonstration of the utility and applicability of deep latent-variable models in observational studies where latent confounding variables pose an omnipresent threat to valid causal inference.","Studies using instrumental variables (IV) often assess the validity of their identification assumptions using falsification tests. However, these tests are often carried out in an ad-hoc manner, without theoretical foundations. In this paper, we establish a theoretical framework for negative control tests, the predominant category of falsification tests for IV designs. These tests are conditional independence tests between negative control variables and either the IV or the outcome. We introduce a formal definition for threats to IV exogeneity as alternative path variables and characterize the necessary conditions that proxy variables for such unobserved threats must meet to serve as negative controls. The theory highlights prevalent errors in the implementation of negative control tests and how they could be corrected. Our theory can also be used to design new falsification tests by identifying appropriate negative control variables, including currently underutilized types, and suggesting alternative statistical tests. The theory shows that all negative control tests assess IV exogeneity. However, some commonly used tests simultaneously evaluate the 2SLS functional form assumptions. Lastly, we show that while negative controls are useful for detecting biases in IV designs, their capacity to correct or quantify such biases requires additional non-trivial assumptions.","Algorithms for constraint-based causal discovery select graphical causal models from among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a chosen functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling procedure, that essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage. Though most of our exposition focuses on the PC algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, the approach is general and modular, so it can be used with other conditional independence based discovery algorithms and (semi-)parametric families.","Inferring the effect of interventions within complex systems is a fundamental problem of statistics. A widely studied approach employs structural causal models that postulate noisy functional relations among a set of interacting variables. The underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies. In a recent line of work, additional assumptions on the causal models have been shown to render this causal graph identifiable from observational data alone. One example is the assumption of linear causal relations with homoscedastic errors that we will take up in this work. When the graph structure is known, classical methods may be used for calculating estimates and confidence intervals for causal effects. However, in many applications, expert knowledge that provides an a priori valid causal structure is not available. Lacking alternatives, a commonly used two-step approach first learns a graph and then treats the graph as known in inference. This, however, yields confidence intervals that are overly optimistic and fail to account for the data-driven model choice. We argue that to draw reliable conclusions, it is necessary to incorporate the remaining uncertainty about the underlying causal structure in confidence statements about causal effects. To address this issue, we present a framework based on test inversion that allows us to give confidence regions for total causal effects that capture both sources of uncertainty: causal structure and numerical size of nonzero effects.","Complications in applied work often prevent point estimation of target quantities using cheap data—at best, sharp bounds can be reported. To make progress, researchers frequently collect more information by (1) re-cleaning existing datasets, (2) gathering secondary datasets, or (3) pursuing entirely new designs. Examples include correcting missingness, recontacting attrited units, validating proxies, finding instruments, and conducting follow-up experiments. These auxiliary tasks are costly, forcing tradeoffs with (4) larger samples from the original approach. Researchers’ choices over these tasks are often based on convenience or intuition. We show how to provably identify the most cost-efficient data-collection strategy. We quantify existing data quality using confidence region width for sharp bounds, which captures two sources of uncertainty: statistical uncertainty from finite samples of measured variables, and fundamental uncertainty because some variables are entirely unmeasured. We show how to compute expected information gain: the expected amount by which each data-collection task will narrow these bounds by addressing one or both sources of uncertainty. Finally, we select the task with the greatest information efficiency, or gain per unit cost. Leveraging recent advances in automatic bounding, we show efficiency is computable for essentially any discrete causal system, estimand, and auxiliary data task. Based on this theoretical framework, we develop a method for optimal adaptive allocation of data-collection resources. Users input a causal graph, estimand, and past data. They enumerate distributions from which future samples can be drawn, fixed and per-sample costs, and any prior beliefs. Our method automatically derives and sequentially updates the optimal data-collection strategy.","The continuity approach to Regression Discontinuity (RD) designs relies on a continuity assumption of the mean potential outcomes at the cutoff defining the RD design. In practice, this assumption is often implausible when changes other than the intervention of interest occur at the cutoff (e.g., other policies are implemented at the same cutoff). When the assumption is implausible, researchers often retreat to ad-hoc analyses that are not justified by any theory and yield results with unclear interpretation. These analyses seek to exploit additional data for which no intervention was applied for all units (regardless of their running variable value). This could be the case when data from multiple time periods are available. We derive the bias of RD designs when the continuity assumption does not hold. We then present a theoretical foundation for analyses using multiple time periods by the means of a general identification framework which incorporates data from additional time periods to overcome the bias. We discuss this framework under various RD designs, and also extend our work to allow for carry-over effects. We derive local linear regression estimators and optimal bandwidths by adapting the best practices for the single period RD to our multiple period setup. The approach is illustrated in the study of the effect of new fiscal laws on debt of Italian municipalities. Starting from 2001, only municipalities with a population above five thousand must adhere to the fiscal law, however, the mayor’s salary also changes at the same cutoff, rendering the continuity assumption implausible.","In spite of their obvious shortcomings, the impact of single arm trials (SAT) is steadily rising: for the 2014-2019 FDA authorised anticancer drugs, one third of the supporting trials were SAT. At the same time Patient Reported Outcomes (PROs) are more often used as major endpoints for evaluating treatments following increased regulatory focus on patients’ perspective. IMI-SISAQOL, a multidisciplinary international consortium, aims to develop thorough and accessible guidance on evaluating PROs in oncology.\nIn high mortality settings the estimand most directly informing patients is arguably ‘QoL while Alive’ (QwAL). Given the inherently bivariate outcome of survival and QoL this treatment effect integrates both a direct impact on QoL and any selection through survival. In either case it informs potential patients on QoL where it matters the most: while they are alive.\nTo derive a treatment effect on QwAL we compare SAT data with external data from the control arm of a pre-existing RCT. Because of stricter inclusion criteria, relying on an RCT typically implies a reduced population scope for the causal contrast. In addition, RCTs may suffer complicated missing data patterns when one treatment is perceived as superior, due to switching from control to treatment . We will present methods to meaningfully transport both the survival and QoL components of QwAL to the target population, either implicitly through weighting or explicitly through regression standardization. Eventually an honest estimate of uncertainty should incorporate between study variability to account for selecting this one specific control out of many possible.","Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data. Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE. However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts. We propose an general framework that accommodates adjustment for any subset of information expressed by the covariates. We generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment. This information is minimally sufficient for prediction of the outcome conditionally on treatment. \nBased on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for the DOPE under general conditions. Compared to the augmented inverse propensity weighted (AIPW) estimator, the DOPE can retain its efficiency even when the covariates are highly predictive of treatment. We illustrate this with a single-index model, and with an implementation of the DOPE based on neural networks, we demonstrate its performance on simulated and real data. Our results show that the DOPE provides an efficient and robust methodology for ATE estimation in various observational settings.","Regulatory actions have been enacted in the United States to diminish the levels of pollutants in the air and reduce the connected environmental risks to health. Indirect accountability studies -assessing the causal effect of exposure to higher levels of air pollution- and direct accountability studies -assessing the causal impact of interventions aimed at reducing the level of air pollution- have found solid evidence of health benefits. However, the existing literature lacks robust methods that consider two crucial points in health studies: evaluate heterogeneity in the health effects of air pollution regulations across different groups of individuals, and consider the joint relations between direct and indirect effects. In this work, we develop a novel approach combining Bayesian nonparametric (BNP) methods and Principal stratification (PS) framework to deal with post-treatment variables that are potentially affected by the treatment and also affecting the response. We introduce two major innovations: (i) we rely on BNP methodologies for the imputation of missing potential outcomes for the post-treatment and outcome variables; (ii) we propose a data-driven methodology to discover causal heterogeneity. We illustrate the performance of the method through simulations. In the application, we discover and estimate the heterogeneous effects of US national air quality regulations on pollution levels and health outcomes.","We describe a design-based framework for drawing causal inference in randomized experiments. Causal effects are defined as linear functionals evaluated at unit-level potential outcome functions. Assumptions about the potential outcome functions are encoded as function spaces. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions, including interference. We describe a class of estimators for estimands defined using the framework and investigate their properties. The construction of the estimators is based on the Riesz representation theorem. We provide necessary and sufficient conditions for unbiasedness and consistency. Finally, we provide conditions under which the estimators are asymptotically normal, and describe a conservative variance estimator to facilitate the construction of confidence intervals for the estimands.","Policy interventions are often evaluated using the difference-in-differences (DiD) approach, which generally estimates treatment effects under a binary exposure. However, in many such interventions, affected units are differentially exposed to the policy. For example, excise taxes may affect taxed units according to their proximity to a non-taxed region and nearby economic competition, both of which also vary according to other demographic and socioeconomic characteristics. Understanding the causal effects of these heterogeneous exposures is an important step to identify effect mechanisms and predict effects under different policy implementations. Existing DiD approaches to estimate effects of continuous exposures are limited by strong parametric and no confounding assumptions. To address these concerns, we present novel methodology to robustly identify causal effects of policy interventions with heterogeneous direct and spillover exposures by merging semi-parametric dose-response and DiD literature. Our methods relax assumptions on model specification and interference while adjusting for confounding between treated and control units as between treated units receiving different exposure levels. After applying this framework to study the Philadelphia sweetened beverage tax policy, we demonstrate preliminary extensions for using these estimates to transport effects to regions with different demographic and exposure distributions that may be considering their own local tax implementations. In the process, we discuss the plausibility of required identification assumptions for effect transportation in policy settings.","In the presence of interference, a unit's outcome might be driven by the treatment level of multiple units. In some settings, the units that receive the treatment are different from the units on which the outcome is measured. Which units' treatment can drive which units' outcomes can be depicted in a bipartite graph which is often assumed to be fixed across time. In this manuscript, we consider the case of estimating causal effects in the presence of bipartite interference from data measured across time and a bipartite interference network that changes over time. We show that in the absence of time-varying confounders, the random bipartite network leads to the exposure randomized with respect to potential outcomes within a temporal window. This allows us to identify the causal effects of a change in the exposure received by an outcome unit while controlling only for temporal trends. Our identification results hold for continuous, binary, and multivariate exposures. In the case of a binary exposure, we propose three matching algorithms to estimate the causal effect based on matching exposed to unexposed time periods. We show that the bias of the proposed algorithms for estimating the causal effect is bounded by algorithmic parameters and the form of temporal trends in the outcome model when time-varying confounding does not exist. We illustrate our approach with an extensive simulation study and an application on the effect of the presence of wildfire smoke on transportation by bicycle.","Causal inference frameworks exist for multistate models of disease progression and are used to estimate treatment effects of cognate therapies. In contrast, drug repurposing studies aim to estimate the causal effect of a drug candidate from another class on the incidence and progression of the disease of interest.\nUsing Electronic Health Records (EHR) and/or claims data retrospectively, target trial methods emulate randomized controlled trials. Emulated trials based on observational data are particularly useful for diseases that develop over multiple decades and have several, clinically-defined stages. Here, using a multistate model of dementia stages, we emulate a target trial and estimate the effect of two antidiabetic drug classes - one hypothesized with parallel anti-aging properties - on the disease transition times.\nBuilding upon the target trial emulation framework (Hernan and Robins, 2016), we combine a parametric estimation procedure (Aastveit, 2023) with a multi-layered temporal causal graph and estimate its parameters via maximum likelihood. Importantly, our method accommodates interval censoring and patients lost to follow up. It can also handle time-varying covariates, by adapting work by Sparling et al (2006).\nWe explore different strategies to incorporate the temporality of the repurposing drug candidate: intention-to-treat in a single patient cohort, cohort stratification by age at treatment initiation, and joint modeling of both diseases within a single multistate model.\nTo implement the latter, we relax the assumption of conditional independence among sequential transition times inside a multistate model of dementia stages together with the treatment initiation time and estimate conditional densities.","Bayesian Causal Forests (BCFs) are designed to estimate heterogeneous treatment effects using observational data by teasing apart the model into 3 pieces: prognostic effect – the influence of the covariates; treatment effect – the influence of the treatment; and propensity score, which captures the treatment assignment mechanism. However, when using data from different sources, the treatment assignment mechanism might differ greatly between each dataset. Additionally, the prognostic and/or treatment effects, as well as the sets of covariates, may also differ. A well-known example arises when combining randomised control trials (RCTs) and observational studies – RCT data tend to include less measurement error and higher internal validity. Combining these data sources can improve many aspects of causal inference, from increased statistical power to better external validity but it is essential to account for the bias introduced by the observational data. Therefore, we extend the BCF model by introducing additional terms in the prognostic and treatment effects, which can absorb differences between two data sources, as well as capture some potential unobserved confounding of the observational data. Additionally, our model introduces a weighting parameter, allowing for adjustment of the influence of the observational data points. During the model fitting process, their contribution to the posterior distribution is raised to a power following a semi-modular inference approach. The flexibility of the power is valuable because it allows us to prevent RCT data from being swamped by the larger possibly confounded observational dataset. We implement our methods on a number of simulated and real data examples.","Electronic health records (EHR) are a common observational data source for informing comparative effects of initiating, and adhering over time, to different pharmacologic treatments for the same disease indication. The Medications and Weight Gain in PCORNet (MedWeight) study is one example. Theory for estimating causal effects in EHR studies with time-varying treatments and confounders is long established. However, practical application of longitudinal causal methods to date has emphasized survival outcomes. An important feature of MedWeight is its primary interest in causal effects on weight change, a continuously measured health marker. Causal inference with health markers like weight change (or cholesterol, blood pressure) pose particular challenges, including that a standard average treatment effect is undefined when “intercurrent” deaths occur. Here, I will highlight differences between survival and continuous outcomes in applied longitudinal causal inference with EHR data and describe an inverse probability weighted estimator that may be useful for studies like MedWeight where: 1) outcomes of interest are continuous health markers, 2) outcomes are sparsely, repeatedly, and informatively measured, and 3) interest is in causal effects of pragmatic time-varying adherence strategies that allow “grace periods” (time off treatment which may be dynamically determined). I will consider a causal effect notion given truncation by death that aligns with the original motivation for MedWeight for two clinical examples: comparative studies of antidepressants on weight change and comparative studies of antihypertensives. I will further consider conditions under which the posed estimator can recover such effects, considerations for practical implementation, and future directions.","Confounding bias and selection bias are two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can arise through informative missingness, wherein relevant information about units in the target population is missing, censored, or coarsened due to factors related to the exposure, the outcome, or their consequences. We extend existing graphical criteria to address selection bias induced by missing outcome data by leveraging post-exposure variables. We introduce the Sequential Adjustment Criteria (SAC), which support recovering causal effects through sequential regressions. A refined estimator is further developed by applying Targeted Minimum-Loss Estimation (TMLE). Under certain regularity conditions, this estimator is multiply-robust, ensuring consistency even in scenarios where the Inverse Probability Weighting (IPW) and the sequential regressions approaches fall short. A simulation exercise featuring various toy scenarios compares the relative bias and robustness of the two proposed solutions against other estimators. As a motivating application case, we study the effects of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data (n=9352). Our findings support the accumulated clinical evidence affirming a positive but small effect of stimulant medication on school performance. A small positive selection bias was identified, indicating that the treatment effect may be even more modest for those exempted or abstained from the tests.","We derive new results on optimal treatment regimes in settings where the effect of interest is only partially identified, i.e., bounded. These results are driven by consideration of an individual's natural treatment value, which is typically ignored in the existing literature on optimal regimes. However, as we show, regimes that explicitly leverage the natural treatment value are guaranteed to outperform conventional optimal regimes. Furthermore, sharp bounds on the new regimes can be obtained from transformations of sharp bounds on conventional optimal regimes. As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the new regimes can be fully identified when conventional optimal regimes are not. We similarly illustrate this property in an instrumental variable setting. Finally, we derive efficient estimators for upper and lower bounds on the value function in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds. These estimators are applied to study the effect of college education on future earnings and of an influenza vaccine on hospitalizations.","Our aim is to estimate causal effects from hypothetical changes of treatment schedules in clinical situations. \nWe present a generic method to calculate efficient influence functions (EIF) for a large class of estimands that that have causal interpretation. This method is based on the underlying martingale structure, and is also heavily inspired by some classic results on smooth manifolds. Treating the EIF as an estimating equation for the targeted estimand, has become one of the standard strategies for estimation in causal inference.\nHowever, it is well known that EIFs usually also depend on some nuisance parameters, not only the estimand of interest. It is important that the EIF is insensitive to fluctuations in the nuisance parameters, if not, this estimation strategy will most likely fail in real life situations. Lack of this property is a common motivation for using TMLE. On the other hand, if this property holds, we can plug in crude machine learning estimates for the nuisance parameters and still get well behaved estimators. We provide a generic method for reparametrizing the counting process model such that this property holds locally. A crucial condition for this method to work is that a particular linear operator has a bounded inverse. It is an open question if TMLE would still converge if this spectral property is violated.","We review the use of generative models and conditional probability for expressing uncertainty in the value of causal estimands using observational data. The predictive nature of Bayesian models highlights the source of statistical uncertainty as the missing data from missing observations. This motivates nonparametric, prior-free, generative methods that align with recent work in target trial emulation. Examples are provided for inferring treatment effects using data from multiple observational and randomized studies from multiple geographic locations.","In this presentation, we focus on causal inference in observational studies with missing values in the covariates. Handling missing data in causal inference requires coupling causal assumptions with assumptions about the mechanisms of missingness. Existing approaches can be classified into two families: the ones that adapt the causal inference assumptions to the missing values setting (employing the assumption of unconfoundedness with missing values) and the ones that consider the classical machinery but missingness mechanisms assumptions. Our focus will be on the latter, specifically emphasizing strategies of \"impute-then-estimate\" causal effects.\nFirst, we study the MAR conditions through the lens of imputation and consider non-parametric imputation methods. Then, we present an identification result for the fully conditional specification (FCS) approach under the weakest MAR assumption. Finally, building upon the aforementioned contributions, we outline four key properties that an effective imputation method must satisfy within the FCS/MICE framework under MAR. An key one is the ability to deal with distributional shifts. Additionally, we explore methods that approximate meeting most of these criteria.","In clinical trials, there is potential to improve precision and reduce the required sample size by appropriately adjusting for baseline variables in the statistical analysis. This is called covariate adjustment. Despite recommendations by regulatory agencies in favor of covariate adjustment, it remains underutilized leading to inefficient trials. We address three potential obstacles that make it challenging to use covariate adjustment. A first challenge is the incompatibility of many covariate-adjusted estimators with standard boundaries in group sequential designs (GSD). GSDs, which involve pre-planned interim analyses for early stopping for efficacy or futility, are commonly used for ethical and efficiency reasons. However, adjusted estimators may lack the independent increments structure (asymptotically) required to directly apply standard stopping boundaries. We address this by applying a linear transformation to the sequence of adjusted estimators across analysis times, resulting in a new sequence of consistent, asymptotically normal estimators with the independent increments property and that either improves or leaves precision unchanged. Second, we address the practical problem of handling uncertainty about how much (if any) precision gain will result from covariate adjustment. This is important for trial planning, since an incorrect projection of a covariate’s prognostic value risks an over- or underpowered trial. We propose using information-adaptive designs, i.e., continuing the trial until the required information level is achieved. This design enables faster, more efficient trials, without sacrificing validity or power. Third, practical implementation of efficient estimators has been hindered by the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms. By enabling the use of data-adaptive methods, while at the same time guaranteeing a valid inference that is insulated against model misspecification, we indicate how covariate adjustment can be incorporated in an automated manner.","We present a necessary and sufficient causal identification criterion based on adjusting for covariates that can be used given maximally oriented partially directed acyclic graphs (MPDAGs). MPDAGs as a class of graphs include directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs), and CPDAGs with added background knowledge. As such, they represent the type of graph that can be learned from observational data and background knowledge under the assumption of no latent variables. In the second part of the talk, we compare the asymptotic efficiency of the estimators obtained through our criterion under certain assumptions. We also derive an optimal estimator.","We propose a test for the identification of causal effects in mediation and dynamic treatment models that is based on two sets of observed variables, namely covariates to be controlled for and suspected instruments, building on the test by Huber and Kueck (2022) for single treatment models. We consider models with a sequential assignment of a treatment and a mediator to assess the direct treatment effect (net of the mediator), the indirect treatment effect (via the mediator), or the joint effect of both treatment and mediator. We establish testable conditions for identifying such effects in observational data. These conditions jointly imply (1) the exogeneity of the treatment and the mediator conditional on covariates and (2) the validity of distinct instruments for the treatment and the mediator, meaning that the instruments do not directly affect the outcome (other than through the treatment or mediator) and are unconfounded given the covariates. Our framework extends to post-treatment sample selection or attrition problems when replacing the mediator by a selection indicator for observing the outcome, enabling joint testing of the selectivity of treatment and attrition. We propose a machine learning-based test to control for covariates in a data-driven manner and analyze its finite sample performance in a simulation study. Additionally, we apply our method to Slovak labor market data and find that our testable implications are not rejected for a sequence of training programs as typically considered in dynamic treatment evaluations.","Advances in AI, statistics, and causal inference, coupled with unprecedented access to complex multimodal data, now allow us to ask more nuanced causal questions, to strengthen and refine identification results,  and to deploy increasingly sophisticated statistical estimators.  Default-based approaches at each of these steps will often fall short; careful fine-tuning for the task at hand is needed to fully realize the potential benefits of the modern data and methodological landscape.   The Causal Roadmap provides a structured framework for implementing the principled context-responsive optimization required.  I will illustrate advances, challenges, and opportunities at each step of the Roadmap using a series of real-world analyses, with an emphasis on longitudinal stochastic interventions. In particular, I will discuss counterfactual stratum effects, using perinatal outcomes in a large pragmatic cluster-randomized trial in East Africa (the SEARCH Study) as a running example. I will close by highlighting emerging areas, including development of \"Causal Co-Pilots\" based on grounding Large Language Models in the Causal Roadmap, and the potential of such co-pilots to democratize and accelerate rigorous causal analyses.","Unmeasured confounding is one of the most important problems for causal inference. Among methodologists, quantitative sensitivity analysis is widely considered a useful approach to handle this problem. However, in applied research, sensitivity analysis is still not commonly used. A notable exception is the sensitivity analysis recently proposed by Ding and VanderWeele, which, in just a few years, has become widely adopted in epidemiological research. In this sensitivity analysis, the analyst specifies certain parameters that quantify the strength of unmeasured confounding, and then use these, together with the observed data distribution, to express bounds for the causal effect of interest. In this presentation, we will review Ding and VanderWeele’s sensitivity analysis, propose improvements, and discuss an important limitation of the method. We will use several real data sets to illustrate key ideas.","TBD"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>N<\/th>\n      <th>Day<\/th>\n      <th>Time<\/th>\n      <th>Room<\/th>\n      <th>Title<\/th>\n      <th>Name<\/th>\n      <th>Affiliation<\/th>\n      <th>Additional Authors<\/th>\n      <th>Abstract<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"responsive":true,"order":[[0,"asc"]],"columnDefs":[{"className":"dt-right","targets":0},{"name":"N","targets":0},{"name":"Day","targets":1},{"name":"Time","targets":2},{"name":"Room","targets":3},{"name":"Title","targets":4},{"name":"Name","targets":5},{"name":"Affiliation","targets":6},{"name":"Additional Authors","targets":7},{"name":"Abstract","targets":8}],"autoWidth":false,"orderClasses":false,"orderCellsTop":true}},"evals":[],"jsHooks":[]}</script>
</div>
</div>





</body></html>